{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSM analysis using structural features single trajectory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>1. Boring stuff first</center>\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 First, imports for analyzing trajectory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new session\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Who needs these!?\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pathlib\n",
    "import dill # save / load sessions to avoid\n",
    "            # processing data multiple times\n",
    "\n",
    "session_loaded = False\n",
    "_notebook_file = pathlib.Path(\".msm-simple-notebook.db\")\n",
    "\n",
    "_model_name = lambda feat, kwargs: \"__\".join([feat] + [\n",
    "    \"%s-%s\"%(str(k),str(v)) for k,v in kwargs.items()])\n",
    "\n",
    "if _notebook_file.is_file():\n",
    "    dill.load_session(_notebook_file)\n",
    "    session_loaded = True\n",
    "\n",
    "if not session_loaded:\n",
    "    # While things are changing alot\n",
    "    # autoreload is super helpful\n",
    "\n",
    "    # Builtins\n",
    "    from pprint import pprint\n",
    "    from itertools import chain\n",
    "\n",
    "    # Installed\n",
    "    import pandas\n",
    "    import seaborn\n",
    "    from scipy.stats import pearsonr\n",
    "\n",
    "    #matplotlib.rcParams['font.family'] = 'serif'\n",
    "    #seaborn.set(font=\"Arial\")\n",
    "    seaborn.set(font_scale=1.7)\n",
    "    seaborn.set_style(\"ticks\", {\"font.family\" : [\"serif\"],\n",
    "                            #\"font.scale\"  : \"1.7\",\n",
    "                            \"font.serif\"  : [\"Times New Roman\"]})\n",
    "\n",
    "    #colorcycle = seaborn.husl_palette(14, l=.5)\n",
    "    #colorcycle = seaborn.cubehelix_palette(12, l=0.5)\n",
    "    #colorcycle = seaborn.color_palette(\"cubehelix\", 12)[2:]\n",
    "    colorcycle = seaborn.color_palette(\"Set2\", 8)\n",
    "    colorcycle2 = seaborn.color_palette(\"cubehelix\", 12)\n",
    "    # For coloring clustercenters onto TICA\n",
    "    # density map with grey-low to pink-high\n",
    "    # color scheme & contours\n",
    "    cc_color = \"blue\"\n",
    "\n",
    "    #prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    #colorcycle = prop_cycle.by_key()['color']\n",
    "\n",
    "    # Mine\n",
    "    import aswa_tools\n",
    "\n",
    "    # This loads a bunch of packages\n",
    "    # under the most common alias\n",
    "    #  - numpy: np, pyplot: plt, etc\n",
    "    #\n",
    "    # TODO use a helper function to\n",
    "    # export common packages to top\n",
    "    # namespace and keep rest of aswa\n",
    "    # bound to parent module name\n",
    "    import aswanalysis as aswa\n",
    "    matplotlib = aswa.plots.matplotlib\n",
    "    plt        = aswa.plots.plt\n",
    "    mdtraj     = aswa.mdtraj\n",
    "    pyemma     = aswa.pyemma\n",
    "    coor       = aswa.coor\n",
    "    np         = aswa.np\n",
    "    msm        = aswa.msm\n",
    "    subprocess = aswa.subprocess\n",
    "    shlex      = aswa.shlex\n",
    "    \n",
    "    print(\"Started a new session\")\n",
    "else:\n",
    "    print(\"Loaded a pre-existing session for this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 File & Dataset Organization for reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Import a python file written specifically to navigate the data we want to process</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from msm_datasets import *\n",
    "\n",
    "# extra helpers for later\n",
    "_steps_to_ns    = lambda s: s * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Load multiple datasets into structure representing the workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Datasets will be \"lined up\" for epoch-to-epoch comparison even if different timestep, n replicates, etc.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not session_loaded:\n",
    "    # filterkey prevents loading unwanted files\n",
    "    #   - the data will be trimmed down and marked with tag \"stride\"\n",
    "    #     if already happened, we don't include in list of source data\n",
    "    aswa.create_trajlist(workflows, filterkey=lambda fnm: \"stride\" not in fnm)\n",
    "    aswa.assign_stride(workflows)\n",
    "    aswa.determine_epochsize(workflows)\n",
    "    aswa.determine_datashape(workflows, atomselection=heavies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Copy datasets to prepare for different analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-10-19 23:36:24 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[0] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n",
      "20-10-19 23:36:24 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[1] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n",
      "20-10-19 23:36:24 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[2] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n",
      "20-10-19 23:36:24 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[3] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n",
      "20-10-19 23:36:24 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[4] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n",
      "20-10-19 23:36:24 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[5] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n",
      "20-10-19 23:36:25 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[6] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n",
      "20-10-19 23:36:25 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[7] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n"
     ]
    }
   ],
   "source": [
    "if not session_loaded:\n",
    "\n",
    "    feature_selections = dict(\n",
    "        invca    = {\"add_inverse_distances\": {\"select_Ca\": None}},\n",
    "        invca_ba = [\n",
    "            {\"add_inverse_distances\": {\"select_Ca\": None}},\n",
    "            #{\"add_chi1_torsions\": None},\n",
    "            {\"add_sidechain_torsions\": None,\n",
    "             \"kwargs\": {\"which\": [\"chi1\"]}},\n",
    "            {\"add_backbone_torsions\": None},\n",
    "    ])\n",
    "    \n",
    "    all_models = dict(\n",
    "        {kk: dict(\n",
    "            anton_longtraj  = {\"atn\": aswa_tools.copy_include(workflows[\"atn\"])},\n",
    "            openmm_longtraj = {\"omm\": aswa_tools.copy_include(workflows[\"omm\"])},\n",
    "            umi_Ca          = {\"omm\": aswa_tools.copy_include(workflows[\"umi_Ca\"])},\n",
    "            umi_CB          = {\"omm\": aswa_tools.copy_include(workflows[\"umi_CB\"])},\n",
    "            #Ca_workflow     = aswa_tools.copy_include(\n",
    "            #    {k:workflows[k] for k in [\"xma_Ca\", \"umi_Ca\"]}),\n",
    "            #CB_workflow     = aswa_tools.copy_include(\n",
    "            #    {k:workflows[k] for k in [\"xma_CB\", \"umi_CB\"]}),\n",
    "            #all_data        = aswa_tools.copy_include(workflows),\n",
    "        ) for kk in feature_selections\n",
    "    })\n",
    "\n",
    "    for feat, featz in feature_selections.items():\n",
    "        for nm, dataset in all_models[feat].items():\n",
    "            dataset[\"data_reader\"], dataset[\"data_order\"] = aswa.prepare_tica_inputs(\n",
    "                dataset, topologies[heavies], features=featz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Define the calculations we will do"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
